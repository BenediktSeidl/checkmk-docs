= Understanding and configuring services
:revdate: 2018-07-05
:title: Detecting and monitoring elements of a host
:description: Services form the core of every monitored host. These are automatically recorded and can be extensively configured and structured.


== Introduction

Services are the actual ‘substance’ of a monitoring system. Each one represents an important
cog in your complex IT landscape. The usefulness of the complete monitoring
stands or falls depending on how accurately and usefully the services have
been configured. Finally, the monitoring should reliably notify whenever
a problem becomes apparent somewhere, but should definitely also minimise
false or useless alarms.

image::bilder/services_illu.png[align=float,left]

(CMK) demonstrates possibly its greatest strength when configuring services:
it possesses an unrivalled and very powerful system for an _automatic
detection and configuration of services_. With Checkmk there is no need
to define every single service via templates and individual allocations.
(CMK) can automatically and reliably detect the list of services to be
monitored, and first and foremost, _keep it up to date_. This not
only saves a lot of time -- it also makes the monitoring _accurate_.
It ensures that the daily changes in a processing centre are always promptly
covered and that no important service goes unmonitored.

The service discovery in Checkmk is based on an important basic principle:
the separation of _what_ from _how_:

* *What* should be monitored? → _The system`/var` on the host `lnxsrv015`_
* *How* should it be monitored? → _at 90% used space (WARN), at 95% (CRIT)_

_What_ is automatically detected by the service discovery. It is
a combination of the *host name* (`lnxsrv015`), the *check
plug-in* (`df:` data system check in Linux) and the *item*
(`/var`). Check plug-ins that can create a maximum of one service on a
host do not require an item (e.g., the check plug-in for CPU utilisation). The
results from a service discovery are presented in a table as shown below:

<table class=small>

<tr>
<th>Host</th>
<th>Checkplugin</th>
<th>Item</th>
</tr>

<tr>
<td class=tt>lnxsrv015</td>
<td class=tt>df</td>
<td class=tt>/</td>
</tr>

<tr>
<td class=tt>lnxsrv015</td>
<td class=tt>df</td>
<td class=tt>/var</td>
</tr>

<tr>
<td class=tt>lnxsrv015</td>
<td class=tt>cpu.util</td>
<td class=tt></td>
</tr>

<tr>
<td class=tt>...</td>
<td class=tt>...</td>
<td class=tt>...</td>
</tr>

<tr>
<td class=tt>app01cz2</td>
<td class=tt>hr_fs</td>
<td class=tt>/</td>
</tr>

<tr>
<td class=tt>...</td>
<td class=tt>...</td>
<td class=tt>...</td>
</tr>

[cols=, ]
|===

The _how_ -- thus the thresholds/check parameters for the individual
services -- is configured independently via [wato_rules|rules]. You can
e.g., define a rule that monitors all data systems with the mount point`/var`,
and the 90%/95% thresholds, without needing to think about on
which hosts such a data system even exists. This is what makes configuring
with Checkmk so clear and simple!

A few services can’t be installed using an automatic discovery. Among these
are e.g., checks queried per HTTP-specified websites. These are created via
rules, as you can learn about [wato_services#active_checks|below].


[#discovery]
== Host services in WATO

=== Incorporating a new host

Once you have added a new host in WATO the next step is to call up the list
of services. With this action the automatic service discovery takes place
for the first time for this host. You can also call up this list at any time later in order to
restart the discovery or to carry out modifications to the configuration. There
are various ways of opening the service list:

* via the ICON[button_save_and_services.png] button or ICON[button_host_services.png] in the host details in WATO
* via the ICON[button_services.png] symbol in the list of hosts in a folder in WATO
* via the ICON[button_services.png] [.guihints]#Edit services# entry in a host’s service’s context menu [wato_services#discovery_check => {{Check_MKDiscovery}}]# 

When a host has been newly-incorporated its services have not yet been
configured, and therefore all discovered services appear in the
[.guihints]#Available (missing) services# category:

image::bilder/wato_available_services.jpg[]

The usual method is to simply save with ICON[button_save_manual.png], followed
by an [.guihints]#Activate changes# -- after which the host will be in the monitoring.


[#available]
=== Adding missing services

For a host that is already being monitored this list looks different. Instead
of [.guihints]#Available (missing) services# you will see [.guihints]#Already configured services}}.# 
If Checkmk detects something on a host that is not being
monitored, that however _should_ be monitored, then the list will look
something like this:

image::bilder/wato_services_missing.jpg[]

The above illustration shows a Windows host on which a filesystem
`D:/` has been found which is not currently being monitored! In
this case it could be that a colleague has created a new LUN.  A click on
ICON[button_activate_missing.png] simply adds all of the missing services
so that the monitoring is again complete. If you only want to add some of
the missing services, you can alternatively select them via the check boxes
and save them with ICON[button_save_manual.png].


=== Vanished services

In processing centres things can not only newly appear, but also disappear.
A data base instance can be discontinued, a LUN unmounted, a file
system removed, etc. Checkmk automatically recognises such services as
_vanished_. In the Service List e.g., it will look like this:

image::bilder/wato_vanished_services.jpg[]

The simplest way to be free of these services is with a click on the
ICON[button_remove_vanished.png] button that appears in such a case.
*Attention*: The reason for the disappearance can of course be due to
a problem! The disappearance of a filesystem can also mean that due to an
error it could not be mounted. The monitoring is after all there for such
cases! You should only remove the service when you really know that this no
longer needs monitoring.


=== Removing unwanted services

You won't necessarily want to monitor everything that Checkmk finds. The
discovery works in a target-oriented way of course, and it can exclude much
unnecessary data in advance. Nonetheless, how can Checkmk know, for example, that
a particular data base instance has been set up only ‘to play around with’,
and is not in production?  There are two ways of eliminating such services:


==== Temporarily disabling services

Use the check boxes to simply deselect the services that are not to be
monitored, and then save with ICON[button_save_manual.png]. And naturally,
don't forget the usual [.guihints]#Activate changes}}...# 

This is however only intended for temporary and smaller actions, as the
services deselected in this way will be highlighted as [.guihints]#missing# by Checkmk,
and the [wato_services#discovery_check|Discovery Check] (which we will show
you later below) will likewise be unhappy. In any case, that would simply
be too much work and not really practical in an environment with x-thousand
services...

[#disabled_services]


==== Permanently disabling services

It is far more elegant and enduring to permanently ignore services with the
aid of the [.guihints]#Disabled services# [wato_rules|rule set]. Here you can not
only exclude individual services from monitoring, but also formulate rules
like “I don’t want to monitor filesystems on test systems that begin with
`/mnt/dsk`”.  The ICON[button_ignore.png] symbol in the service bar
simplifies the creation of such new rules, so that you don't need to take the
longer route via the ICON[icon_rulesets.png] [.guihints]#Host & Service parameters}}# 
WATO-Modul:

image::bilder/disable_services_1.png[]

ICON[button_ignore.png] Takes you directly to the creation of a new rule
that will be prepopulated for the current folder, host and service:

image::bilder/disable_services_2.jpg[]

You can very easily generalise this rule for all hosts: simply remove the
‘check’ at [.guihints]#Specify explicit host names# and -- importantly -- set the
[.guihints]#Folder# to [.guihints]#Main directory}}.#  Naturally, as always, you can formulate
all other desired conditions in the [wato_rules|Rules].

Once you have saved the rules, and return to the host’s service list, you
will discover the new table [.guihints]#Disabled services (configured away by admin)}},# 
This documents all services that have been “shut down” in this way, as shown below:

image::bilder/disable_services_3.jpg[]


[#refresh]
=== Refreshing services

There are a number of plug-ins that _notice_ things during a discovery.
For example, the plug-in for network interfaces checks the speed set on the
interface during the discovery.  Why? In order to be able to warn you in
case it changes! It is rarely a good sign when an interface is sometimes
set to 10MBit, and sometimes to 1GBit -- this could rather be an indication
of a defective autonegotiation.

What happens when this change is desired and is to be accepted as OK from
now on?

Either -- remove the service via the check box (you will need to save after
the removal), and re-add it later.

Or -- click on ICON[button_tabula_rasa.png] -- with this _all_ of the host’s
services will be refreshed and newly-identified. This is naturally much easier,
but only when you don’t want to keep individual services in an error state.


[#snmp]
=== Special conditions with SNMP

There are a few special features for devices that are monitored via SNMP.
You can learn about these in the [snmp#services|Article about SNMP].


[#bulk_discovery]
== Bulk Discovery -- simultaneous discovery on multiple hosts


If you want to perform a discovery for multiple hosts with a single action,
you can make the work easier with WATO’s
[wato_hosts#bulk_operations|Bulk Operations]. Firstly, choose the hosts on which the discovery is to be
performed.  You have several options for this:

. In a folder, select the check boxes for individual hosts and press ICON[button_discovery.png]
. Search for hosts with [wato_hosts#search|Host search], and then press ICON[button_discovery.png] in the search results
. Click on ICON[button_bulk_discovery.png] in a folder

With the third variant you can also perform the service discovery recursively
in all subfolders. In all of the above three options the next step will
take you to the following dialogue:

image::bilder/wato_bulk_discovery_form.jpg[align=center,width=500]

In [.guihints]#Mode# you will find exactly the same options as in the WATO service
list that we have previously discussed.

Under [.guihints]#Selection# you can again control the host selection. This is
primarily sensible if you have selected these via the folder rather than via
the check boxes.  Most of the options are intended to accelerate the discovery:

[cols=, ]
|===


|{{Only include hosts that failed on previous discovery}}
|Hosts for which an earlier service discovery via bulk operations has
failed (e.g. because the host was not accessible at the time), are flagged
with the ICON[icon_inventory_failed.png] symbol.  This option allows the
discovery to be repeated only for these hosts.


|{{Only include hosts with a failed discovery check }}
|This restricts the discovery to such hosts for which the
[wato_services#discovery_check|Discovery Check] failed. When you work with
Discovery Check this is a good method for greatly-accelerating a discovery on
many hosts.  The combination with the {{Refresh all services (tabula rasa)}}
option makes less sense in this case however as it can distort the status
of existing services.


|{{Exclude hosts where the agent is unreachable}}
|Hosts that are not accessible cause long delays during discovery due to
connection timeouts. This can greatly-impede a discovery’s performance on
larger numbers of hosts. If the hosts are already in monitoring -- and it
knows that the hosts are (DOWN) -- you can bypass them here and thus avoid
the timeouts.

|===

The [.guihints]#Performance Options# are predefined so that a [.guihints]#Full Scan}}# 
is always performed on SNMP devices.  If you are not interested in new
plug-ins a discovery can be greatly-accelerated by not choosing this option.
Working without cache data is only advisable in exceptional cases. Especially
for hosts that are monitored using Checkmk agents -- as luck will have
it -- it can occur that log messages are ‘consumed’ by the discovery and not
be received by the production check.

The `10` set in [.guihints]#Number of hosts to handle at once# means that
ten hosts are always processed in one action. This is achieved internally
with a HTTP request. If you encounter timeout problems due to some hosts
requiring a long time to discover, you can try setting this number lower
(to the detriment of the total time required).

As soon as you confirm the dialogue the procedure will start and you can
observe its progress -- and also interrupt it if necessary:

image::bilder/wato_bulk_discovery_progress.jpg[align=center,width=350]


[#parameters]
== Check parameters in services

Many of the check plug-ins can be configured using parameters. The most common
practice is the setting of thresholds for (WARN) and (CRIT). Parameters can
be composed much more complicatedly however, as shown in this example of
temperature-monitoring with Checkmk:

image::bilder/temperature_levels.png[align=center,width=350]

The check parameter for a service is composed of three parts:

. Every plug-in has a _Default value_ for the Parameter.
. Some plug-ins set values during a discovery (see [wato_services#refresh|above]).
. Parameters can be set via rules.

Parameters from rules have priority over those set by a discovery, and these
in turn have priority over default values. For complex parameters in which
individual sub-parameters are set using check boxes (as with temperatur
for example), these priority-rules apply separately for each sub-parameter.
So, if you set only one sub-parameter via rules, the others retain their
respective default values.  In this way you can, for example, activate the trend
calculation of the temperatures with one rule, and with another rule set
the temperature threshold value for a physical sensor device.  The complete
parameter set will then be composed from both rules.

The exact parameters a service eventually has can be found in the service’s
parameter page.  This can be accessed via the ICON[button_check_parameters.png]
symbol in the host’s service list.  If you wish to see the parameters
from all services directly in the service table, you can show it with the
ICON[button_show_check_parameters.png] button.  It will look something
like this:

image::bilder/wato_check_parameters.png[]


== Customising the service discovery

We have [wato_services#disabled_services|earlier] shown how you can configure the
service discovery to suppress the displaying of undesired services.
In addition there are further rule sets for a number of plug-ins
that influence the behaviour of the discovery with these plug-ins.  Not only
are there settings for _omitting_ items, there are also those that
actively find items, or collect them into groups. The naming of items
is sometimes also an issue -- e.g. for those switchports where you can decide
on a description or alias to be used as an item (which will be used in the
service name) instead of its interface ID.

All rule sets that are relevent for service discovery can be found under
ICON[icon_rulesets.png]
[.guihints]#Host & Services parameters => Parametersfor discovered services => Discovery-- automatic service detection}}.# 
Please don’t confuse these rule sets with those intended for parameterising
the actual services. A number of plug-ins have two rule sets in fact -- one
for the discovery, and one for the parameters.  Here are a few examples.


[#processes]
=== Monitoring of processes

It would make little sense for Checkmk to simply define a service to monitor
every process found on a host. Most processes are either of no interest
or are only temporarily present. At the very least there are hundreds of
processes running on a typical Linux server.

For monitoring services you therefore need to work with
[wato_services#manual_checks|Manual checks] or -- and this is much more
elegant -- by using the rule set [.guihints]#Process discovery# to tell the service
discovery which processes it should be on the lookout for.  In this manner
you can always allow a monitoring to be instituted automatically when a
_definitely interesting_ process is found on a host.

The following image shows a rule in the [.guihints]#Process discovery# rule set which
searches for processes that execute the program `/usr/sbin/apache2`.
In this example a service ({{Grab user from found processes}})# will be
created for every different operating system user for whom such a process
is found.  The service will be named `Apache %u`, where `%u`
will be replaced by the user name.  For the threshold the number of process
instances will be set to 1/1 (minimum) and 30/60 (maximum) respectively:

image::bilder/process_discovery.jpg[]

Please note that the predefined thresholds are referred to as
[.guihints]#Default parameters for detected services}}.# You can assign these -- and likewise all
other services -- via rules. As a reminder: the above rules configure the
service _discovery_ -- the _what_. If the services are present
for the first time, the rule chain [.guihints]#State and count of processes# is
responsible for the thresholds.

The fact that you can set thresholds during a discovery is an aid to
convenience. There is a catch though: changes to the discovery rule only
take effect with the _next discovery_. If you change thresholds you
will need to run a new discovery. If, however, you only use the rule to
discover the services (the _what_), and the rule set
[.guihints]#State and count of processes# for the _how_, then you will not have
this problem.

Further information on process discovery can be found in the online help
ICON[icon_help.png] for this rule set.


=== Monitoring services under Windows

The discovery and parameterising of the monitoring of Windows services is
analogous to the processes and is controlled via the rule sets
[.guihints]#Windows Service Discovery# (_what_) and [.guihints]#Windows services# (_how_)
respectively. Here is an example of a rule that watches out for two services:

image::bilder/windows_services_discovery.jpg[]

Exactly as for the processes, here the service discovery is also only
one option. If, on the basis of host characteristics and folders, you
can formulate precise rules for hosts on which specific services are to be
expected, then you can also work with [wato_services#manual_checks|manual services].
This is independent of the situation actually found -- it can however
require considerably more effort, as under these circumstances
you need many rules in order to exactly describe which service is to be
expected on which host.


[#switches]
=== Monitoring of switch ports

(CMK) uses the same logic for monitoring network interfaces on servers
and ports on ethernet switches. With switch ports the existing options for
controlling the service discovery are especially interesting, even though
(in contrast to the processes and Windows services) the discovery initially
functions without rules. That is to say, by default Checkmk automatically
monitors all physical ports that currently have an UP state. The applicable
rule set is called [.guihints]#Network Interface and Switch Port Discovery# and offers
numerous setting options that are only briefly described here:

image::bilder/switch_port_discovery.jpg[]

The following options are the most important:

* The use of the [.guihints]#Description# or the [.guihints]#Alias# in service names
* The restriction or _expansion_ of the types or names of interfaces being monitored


[#manual_checks]
== Setting-up services manually

There are some situations in which an automatic service discovery would make
no sense.  This is always the case if you want to force compliance with a
specific _guideline_.  As we saw in the previous chapter, you can allow
the monitoring of Windows services to set itself up automatically when these
are found. What happens when the absence of such a service presents a problem?
For example:

* A particular virus scanner should be installed on every Windows host.
* NTP should be configured on every Linux host.

In such cases you can install the services manually. The starting point
for this is the ICON[icon_static_checks.png] [.guihints]#Manual Checks# WATO
module. Underlying this is a collection of [wato_rules|Rule sets] which have
exactly the same names as the rule sets used for configuring the parameters
for these checks.

The rules differ in two points however:

* These are rules for _hosts_, not for services. The services will be created by the rules
* Since no discovery takes place, you must select the check plug-in to be used for the check

The following example shows the body of the [.guihints]#State of NTP time synchronisation}}# 
rule under [.guihints]#Manual Checks}}:# 

image::bilder/manual_check_ntp.jpg[]

Alongside the thresholds, here you set the check plug-in (e.g. `chrony`
or `ntp.time`).  For check plug-ins that require an item you must also
specify these. For example, this is necessary for the [.guihints]#oracle_processes}}# 
plug-in, which requires the details of the data base SID to be monitored:

image::bilder/manual_check_oracle_processes.jpg[]

A manual service defined in this way will be installed on all hosts to
which these rules apply.  There will now be three possible conditions for
the actual monitoring:

. The host is correctly installed and the service is (OK).
. The agent notifies that the requested service does not run or has a problem. The service then flags (CRIT) or (UNKNOWN).
. The agent provides no information at all, e.g., because NTP is not even installed. The service then remains in (PEND) and the Checkmk service goes into (WARN) with the notice that the relevent section in the agent data is missing.

You will never require most of the rule sets in the
ICON[icon_static_checks.png] [.guihints]#Manual Checks# module, they are only present
for the sake of completeness.  The most common cases of manual checks are:

* Monitoring of Windows services (Rule set: [.guihints]#Windows Services}})# 
* Monitoring of processes (Rule set: [.guihints]#State and count of processes}})# 


[#discovery_check]
== The discovery check

In the introduction we promised that Checkmk not only detects the list of
services automatically, it can also _keep it up to date_.  It would also
be natural to have the possibility of manually running a bulk discovery for
all hosts from time to time.


[#discovery_auto]
=== Automatic check for unmonitored services

Much better for this however is a regular _Discovery Check_,
which is set up automatically on new instances (from Checkmk version VERSION[1.2.8]).
This service exists for every host and will log a
warning whenever it finds unmonitored items:

image::bilder/discovery_check_warn.png[align=border]

The details of unmonitored or vanished services can be found in the
[.guihints]#Long output of check plugin# in the details of the service:

image::bilder/discovery_check_long_output.png[]

The host’s server list in WATO can be easily accessed via the Discovery
Check's ICON[icon_menu.png] context menu using the ICON[icon_services.png]
[.guihints]#Edit services# entry.

If your instance has been updated from an older version you must install
this check manually.  The installation and parameterising of the Discovery
Check is very simply done using the [.guihints]#Periodic service discovery# [wato_rules|Rule set].
In the rule’s parameter area you have the following installation
options: BI:periodic_service_discovery.jpg

With SNMP devices, alongside the interval in which the check is to be run,
and the monitoring state for cases of unmonitored or vanished services,
you can also select whether a [wato_services#snmp|SNMP-Scan] should take place.


=== Adding services automatically

Missing services can be added automatically to the Discovery Check. To this
end activate the [.guihints]#Automatically update service configuration# option,
which will make further options available.

image::bilder/discovery_check_activate.png[]

Alongside the additions, in [.guihints]#Mode# you can also choose to delete superfluous
services, or even to delete all existing services and perform a complete new
discovery ({{Refresh}}).#  Both options should be used carefully! A vanished
service can indicate a problem!  The Discovery Check will simply delete such
a service and lull you into thinking everything is in order.  The refresh
is especially risky.  For example, the check for switchports will only take
ports that are ‘up’ into the monitoring.  Ports with a status of ‘down’
will be perceived as vanished and quickly deleted from the Discovery Check!

A further problem needs to be considered: adding services or even the
automatic [.guihints]#Activate Changes# can distract you -- the admin -- when you are
performing a configuration.  It can theoretically occur that while you are
working on rules and settings, in that moment a discovery check activates
your changes.  The [wato|WATO] can only always activate all changes!  In order
to preclude such situations you can reschedule the time for this function,
to overnight for example.  The above image shows an example of this.

The [.guihints]#Group discovery and activation for up to# setting ensures that
not every single service that has been newly-found immediately triggers an
[.guihints]#Activate Changes# -- rather there will be a specified waiting time so that
multiple changes can be activated in a single action.  Even if the discovery
check is set to an interval of two hours or more, this only applies to each host separately.
The checks don’t run simultaneously for every host -- which
is a good thing, as a discovery check requires significantly more resources
than a normal check.


[#passive_checks]
== Passive services

Passive services are those that are not actively initiated by Checkmk, rather
by check results regularly channelled from external sources.  This generally
occurs via the core’s command pipe.  Here is a step-by-step procedure for
creating a passive service:

Nextly, you need to notify the core of the service. This is done with the
same rule set as in your [wato_services#legacy_checks|own active checks],
except that you omit the [.guihints]#Command line}}:# 

image::bilder/passive_checks.png[align=center,width=500]

The image also shows how you can verify if check results are being regularly
received.  If these fail to appear for longer than ten minutes then the
service will be automatically flagged as (UNKNOWN).

After an [.guihints]#Activate Changes# the new service will start its life in the
(PEND) state:

image::bilder/passive_check_pending.png[]

Sending the check result now takes place on the command line via an
`echo` of the `PROCESS_SERVICE_CHECK_RESULT` command in the
`~/tmp/run/nagios.cmd` command pipe.

The syntax conforms to the usual Nagios conventions -- including a current
time stamp in square brackets. As the argument with the command you need
the host name (e.g., `myhost123`) and the selected service name
(e.g., `BAR`).  The two subsequent arguments are again the status
(`0` ... `3`) and the plug-in’s output.  The time stamp is
created with `$(date +%s)`:

[source,bash]
----
OMD[mysite]:~$ echo "[$(date +%s)] PROCESS_SERVICE_CHECK_RESULT;myhost123;BAR;2;Something bad has happened" > ~/tmp/run/nagios.cmd
----

The service now immediately shows its new status:

image::bilder/passive_check_crit.png[]

If you are familiar with the Nagios _NSCA_ tool, you can continue
using it with Checkmk as well.  Activate the NSCA receiver with `omd
config`, and as needed modify the NSCA configuration, which is found
under `etc/nsca/nsca.cfg`:

[source,bash]
----
OMD[mysite]:~$ omd stop
OMD[mysite]:~$ omd config set NSCA on
OMD[mysite]:~$ omd config set NSCA_TCP_PORT 5667
OMD[mysite]:~$ vim etc/nsca/nsca.cfg
OMD[mysite]:~$ omd start
----

The system is now ready to receive passive check results via NSCA.


[#commandline]
== Service discovery on the command line

A GUI is fine, but the good old command line is sometimes still
practical -- whether it is for automation or it simply enables an experienced user
to work quickly.  A service discovery can be triggered with the `cmk
-I` command on the command line.  There are a couple of variables in
this process. For all of these the `-v` option is recomended, so that
you can see what happens. Without `-v` Checkmk behaves like the good
old traditional Unix -- as long as everything is ok it says nothing.

With a simple ‘`-I`’ search for *all* hosts by new services:

[source,bash]
----
OMD[mysite]:~$ cmk -vI
switch-cisco-c4000:
nothing new

switch-cisco-c4500:
nothing new

switch-cisco-c4500-2:
nothing new

switch-cisco-c4500-3:
 nothing new
----

With the `-I` you can also enter one or more host names in order
to only discover these.  This additionally has a second effect -- whereas
an `-I` on all hosts basically works only with *cached* data,
(CMK) always works with *fresh* data from an explicitly-nominated host!

[source,bash]
----
OMD[mysite]:~$ cmk -vI myhost123
----

Alternatively, you can filter using tags:

[source,bash]
----
OMD[mysite]:~$ cmk -vI @mytag
----

This would perform the discovery for all hosts with the host tag `mytag`.
Filtering with tags is available for all cmk options that accept multiple hosts.

With the `--cache` and respectively `--no-cache` options you
can explicitly determine the use of cache.

Additional outputs can be received with a second `-v`. With SNMP-based
devices you can even see every single OID retrieved from the device:

[source,bash]
----
OMD[mysite]:~$ cmk -vvI myswitch123
Discovering services on myswitch123:
myswitch123:
 SNMP scan:
       Getting OID .1.3.6.1.2.1.1.1.0: Executing SNMP GET of .1.3.6.1.2.1.1.1.0 on switch
=> ['24G Managed Switch'] OCTETSTR
24G Managed Switch
       Getting OID .1.3.6.1.2.1.1.2.0: Executing SNMP GET of .1.3.6.1.2.1.1.2.0 on switch
=> ['.1.3.6.1.4.1.11863.1.1.3'] OBJECTID
.1.3.6.1.4.1.11863.1.1.3
       Getting OID .1.3.6.1.4.1.231.2.10.2.1.1.0: Executing SNMP GET of .1.3.6.1.4.1.231.2.10.2.1.1.0 on switch
=> [None] NOSUCHOBJECT
failed.
       Getting OID .1.3.6.1.4.1.232.2.2.4.2.0: Executing SNMP GET of .1.3.6.1.4.1.232.2.2.4.2.0 on switch
=> [None] NOSUCHOBJECT
failed.
----

A complete renewal of the services (tabula rasa) can be performed with a
double `-II`:

[source,bash]
----
OMD[mysite]:~$ cmk -vII myhost123
Discovering services on myhost123:
myhost123:
    <b class=green>1* cpu.loads
    <b class=green>1* cpu.threads
    <b class=green>6* cups_queues
    <b class=green>3* df
    <b class=green>1* diskstat
    <b class=green>3* kernel
    <b class=green>1* kernel.util
    <b class=green>3* livestatus_status
    <b class=green>1* lnx_if
    <b class=green>1* lnx_thermal
----

You can also restrict all of this to a single check plug-in. For this the
option is `--checks=`, and it must be placed before the host name:

[source,bash]
----
OMD[mysite]:~$ cmk -vII --checks=df myhost123
Discovering services on myhost123:
myhost123:
    <b class=green>3* df
----

When you are finished you can activate the changes with `cmk -O`
(`cmk -R` with Nagios Core):

[source,bash]
----
OMD[mysite]:~$ cmk -O
Generating configuration for core (type cmc)...OK
Packing config...OK
Reloading monitoring core...OK
----

And when you encounter an error during a discovery...

[source,bash]
----
OMD[mysite]:~$ cmk -vII --checks=df myhost123
  <b class=yellow>WARNING:* Exception in discovery function of check type 'df': global name 'bar' is not defined
  nothing
----

... with an additional `--debug` you can produce a detailed Python
stack trace of the fault location:

[source,bash]
----
OMD[mysite]:~$ cmk --debug -vII --checks=df myhost123
Discovering services on today:
today:
Traceback (most recent call last):
  File "/omd/sites/heute/share/check_mk/modules/check_mk.py", line 5252, in <module>
    do_discovery(hostnames, check_types, seen_I == 1)
  File "/omd/sites/heute/share/check_mk/modules/discovery.py", line 76, in do_discovery
    do_discovery_for(hostname, check_types, only_new, use_caches, on_error)
  File "/omd/sites/heute/share/check_mk/modules/discovery.py", line 96, in do_discovery_for
    new_items = discover_services(hostname, check_types, use_caches, do_snmp_scan, on_error)
  File "/omd/sites/heute/share/check_mk/modules/discovery.py", line 677, in discover_services
    for item, paramstring in discover_check_type(hostname, ipaddress, check_type, use_caches, on_error):
  File "/omd/sites/heute/share/check_mk/modules/discovery.py", line 833, in discover_check_type
    discovered_items = discovery_function(info)
  File "/omd/sites/heute/share/check_mk/checks/df", line 91, in inventory_df
    foo = bar
NameError: global name 'bar' is not defined
----


=== Overview of options

To recap -- all options at a glance:

[cols=, ]
|===

|`cmk -I`
|Discover new services


|`cmk -II`
|Delete and rediscover all services (tabula rasa)


|`-v`
|Verbose: display hosts and detected services


|`-vv`
|Very verbose: display a  precise protocol of all operations


|`--checks=foo`
|Execute a discovery (and also a tabula rasa) only for the specified check plug-in


|`@foo`
|Execute a discovery (and also a tabula rasa) only for hosts with the specified tag


|`--cache`
|Force the use of cache data (normally the default only when no host is specified)


|`--no-cache`
|Fetch fresh data (normally the default only when a host name is specified)


|`--debug`
|Cancel in an error situation, and display the complete Python stack trace


|`cmk -O`
|Activate changes ((EE) with CMC as Core)


|`cmk -R`
|Activate changes ((RE) with Nagios as Core)
|===


=== Saving in files


The _result_ of a service discovery -- thus, as explained earlier, the
tables of host names, check plug-ins, items and identified parameters -- can
be found in the `var/check_mk/autochecks` folder.  Here, for every
host there is a data set that stores the automatically-discovered services.
As long as you don’t damage this data set’s Python syntax you can alter or
delete individual lines manually.  Deleting the data set removes all services
and flags them as quasi ‘unmonitored’ again.

.var/check_mk/autochecks/myhost123.mk

----[
  ('cpu.loads', None, cpuload_default_levels),
  ('cpu.threads', None, threads_default_levels),
  ('diskstat', u'SUMMARY', diskstat_default_levels),
  ('kernel', u'Context Switches', kernel_default_levels),
  ('kernel', u'Major Page Faults', kernel_default_levels),
  ('kernel', u'Process Creations', kernel_default_levels),
  ('kernel.util', None, {}),
  ('livestatus_status', u'stable', {}),
  ('lnx_if', u'2', {'state': ['1'], 'speed': 0}),
  ('lnx_thermal', u'Zone 0', {}),
  ('mem.linux', None, {}),
  ('mknotifyd', u'today', {}),
  ('mknotifyd', u'stable', {}),
  ('mounts', u'/', [u'data=ordered', u'errors=remount-ro', u'relatime', u'rw']),
  ('ntp.time', None, ntp_default_levels),
  ('omd_apache', u'stable', None),
  ('tcp_conn_stats', None, tcp_conn_stats_default_levels),
  ('uptime', None, {}),
]
----




== Service groups in wato_services


=== Why have service groups?


So far you have learned how to include services in monitoring.
Now it makes little sense to have to look at lists of thousands of services and/or always
have to go through host views. For example, if you want to view all file system or
update services together, you can simply assemble groups in a similar way as you can
with [wato_hosts#hostgroups|host groups].

Service groups make it easy for you to bring a lot more order to monitoring via [views|views]
and NagVis maps, and to switch targeted [notifications|notifications] and
[alert_handlers|alert handlers].
By the way – you could almost always construct corresponding views purely using
the view filters – but service groups are more clearly arranged and easier to work with.


=== Creating service groups



Service groups can be found at [.guihints]#WATO => Host & Service Groups}}.# 
By default the host groups appear here, so first click on ICON[button_service_groups.png].
There you will find a similar menu with which the service groups can then be defined:

image::bilder/servicegroups_list2.png[]

Creating a service group is simple: Create a group via ICON[button_new_service_group.png]
and assign a name that cannot be subsequently changed, and likewise a meaningful alias:

image::bilder/servicegroups_config.png[]




=== Adding services to a service group


To assign services to service groups you need the [wato_rules#conditions|rule set]
found under [.guihints]#WATO => Host & Service Parameters => Grouping}}}.# 
Now use ICON[button_create_rule_in_folder.png] to create a new rule in the desired folder.
First you specify which service group to assign services to,
for example _myservicegroup_ or its alias _My Service Group 1.</I>


image::bilder/servicegroups_rule_assignment.png[]

The exciting part now follows in the [.guihints]#Conditions# section. On the one hand,
you can use folders, host tags, and explicit host names to make restrictions
outside of the services. Secondly, you name the services you would like to group,
such as _Filesystems_ and _CIFS mount_ to create a set of file systems.
The specification of the services takes place here in the form of
[regexes|regular expressions]. This allows you to define groups exactly.

image::bilder/servicegroups_rule_conditions.png[]




=== Checking the service groups for a service



You can check the assignment of services on the detail page of a particular service.
Below, by default, is the [.guihints]#Service groups the service is member of# line.

image::bilder/servicegroups_service_detail.png[]




=== Using service groups

As already mentioned, the service groups are used in several places: [views|views],
NagVis maps, [notifications|notifications] and [alert_handlers|alert handlers.]
For new views it is important that you use the [.guihints]#Servicegroups# as the data source.
Of course, the [.guihints]#Views# widget also contains predefined views for service groups,
for example a clear summary:

image::bilder/servicegroups_view_summary.png[]

With a click on the service group names you will receive a complete view of all
of the services of the respective group.

If you use service groups in NagVis maps, you will receive a summary of service
groups opened in a menu by hovering over a single icon:

image::bilder/servicegroups_nagvis.png[]



When you use service groups in [notifications|notifications] and
[alert_handlers|alert handlers], they are available as
[wato_rules#conditions|conditions/filters], of which you can use one or more:


image::bilder/servicegroups_notification_rule2.png[]


[#checkplugins]
== More on Check plug-ins

=== A short description of their functionality

Check plug-ins are required to generate services in Checkmk.
Each service uses a check plug-in to determine its status, create/maintain metrics, etc.
When doing so such a plug-in can create one or more services per host.
So that multiple services from the same plug-in can be distinguished, an *Item* is needed.
For example, for the service `Filesystem /var` the Item is the text `/var`.
In the case of plug-ins that can only generate a maximum of one service per host,
`CPU utilization`) for example, the Item is empty and not shown.


=== Available check plug-ins

A list of all available check plug-ins can be found under [.guihints]#WATO => Check Plugins}}.# 
Here the individual plug-ins can be searched for, filtered in various categories:

image::bilder/wato_services_checkplugins_01.png[]

For each plug-in three columns of information will be shown: a description of the service (Type
of Check), the name of the check plug-in (Plug-in Name) and its compatible data sources (Agents):

image::bilder/wato_services_checkplugins_02.png[]
